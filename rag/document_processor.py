#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æ¡£å¤„ç†æ¨¡å— - æ”¯æŒPDF/TXT/CSV/JSONæ–‡æ¡£çš„å¤„ç†å’Œå‘é‡åŒ–
å¤„ç†æµç¨‹ï¼š
1. æå–æ–‡æœ¬ï¼ˆPDFæ”¯æŒOCRè¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼‰
2. æŒ‰chunk_size=250, overlap=50åˆ‡åˆ†
3. å‘é‡åŒ–åå­˜å…¥ChromaDB
"""

import os
import json
import csv
import hashlib
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime

import fitz  # PyMuPDF
import numpy as np
from tqdm import tqdm


@dataclass
class DocumentChunk:
    """æ–‡æ¡£åˆ†å—"""
    content: str
    metadata: Dict
    chunk_index: int


@dataclass
class ProcessedDocument:
    """å¤„ç†åçš„æ–‡æ¡£"""
    doc_id: str
    filename: str
    file_type: str
    chunks: List[DocumentChunk]
    total_chars: int
    created_at: str


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(
        self,
        chunk_size: int = 250,
        chunk_overlap: int = 50,
        use_ocr: bool = True,
        ocr_threshold: Tuple[float, float] = (0.3, 0.3)
    ):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        
        Args:
            chunk_size: åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap: åˆ†å—é‡å å¤§å°
            use_ocr: æ˜¯å¦å¯¹PDFä¸­çš„å›¾ç‰‡è¿›è¡ŒOCR
            ocr_threshold: OCRå›¾ç‰‡å°ºå¯¸é˜ˆå€¼ (å®½åº¦æ¯”ä¾‹, é«˜åº¦æ¯”ä¾‹)
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.use_ocr = use_ocr
        self.ocr_threshold = ocr_threshold
        self.ocr_engine = None
        
        if use_ocr:
            self._init_ocr()
    
    def _init_ocr(self):
        """åˆå§‹åŒ–OCRå¼•æ“"""
        try:
            from rapidocr_onnxruntime import RapidOCR
            self.ocr_engine = RapidOCR()
            print("âœ… RapidOCR åˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            print("âš ï¸  RapidOCR æœªå®‰è£…ï¼Œå°†è·³è¿‡å›¾ç‰‡OCR")
            print("   å®‰è£…å‘½ä»¤: pip install rapidocr-onnxruntime")
            self.ocr_engine = None
    
    def process_file(self, file_path: str) -> ProcessedDocument:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            ProcessedDocument: å¤„ç†åçš„æ–‡æ¡£å¯¹è±¡
        """
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        file_type = file_path.suffix.lower().lstrip('.')
        filename = file_path.name
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©å¤„ç†æ–¹æ³•
        if file_type == 'pdf':
            text = self._extract_pdf(str(file_path))
        elif file_type == 'txt':
            text = self._extract_txt(str(file_path))
        elif file_type == 'json':
            text = self._extract_json(str(file_path))
        elif file_type == 'csv':
            text = self._extract_csv(str(file_path))
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
        
        # åˆ‡åˆ†æ–‡æœ¬
        chunks = self._split_text(text, filename, file_type)
        
        # ç”Ÿæˆæ–‡æ¡£ID
        doc_id = hashlib.md5(f"{filename}_{datetime.now().isoformat()}".encode()).hexdigest()[:12]
        
        return ProcessedDocument(
            doc_id=doc_id,
            filename=filename,
            file_type=file_type,
            chunks=chunks,
            total_chars=len(text),
            created_at=datetime.now().isoformat()
        )
    
    def _extract_pdf(self, file_path: str) -> str:
        """
        æå–PDFæ–‡æœ¬ï¼ˆæ”¯æŒOCRï¼‰
        
        ä½¿ç”¨fitzæå–æ–‡æœ¬ï¼Œå¯¹å›¾ç‰‡ä½¿ç”¨OCRè¯†åˆ«
        """
        doc = fitz.open(file_path)
        all_text = []
        
        print(f"ğŸ“„ å¤„ç†PDF: {file_path}")
        print(f"   æ€»é¡µæ•°: {doc.page_count}")
        
        for page_num, page in enumerate(doc):
            print(f"   å¤„ç†é¡µé¢ {page_num + 1}/{doc.page_count}...", end="\r")
            
            # æå–é¡µé¢æ–‡æœ¬
            page_text = page.get_text("text")
            all_text.append(page_text)
            
            # å¦‚æœå¯ç”¨OCRï¼Œå¤„ç†é¡µé¢ä¸­çš„å›¾ç‰‡
            if self.use_ocr and self.ocr_engine:
                img_text = self._ocr_page_images(doc, page, page_num)
                if img_text:
                    all_text.append(img_text)
        
        print(f"\n   âœ… PDFæ–‡æœ¬æå–å®Œæˆ")
        doc.close()
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        full_text = "\n".join(all_text)
        
        # æ¸…ç†æ–‡æœ¬
        full_text = self._clean_text(full_text)
        
        return full_text
    
    def _ocr_page_images(self, doc, page, page_num: int) -> str:
        """å¯¹é¡µé¢ä¸­çš„å›¾ç‰‡è¿›è¡ŒOCR"""
        import cv2
        from PIL import Image
        
        ocr_texts = []
        img_list = page.get_image_info(xrefs=True)
        
        for img_info in img_list:
            xref = img_info.get("xref")
            if not xref:
                continue
            
            bbox = img_info["bbox"]
            # æ£€æŸ¥å›¾ç‰‡å°ºå¯¸æ˜¯å¦è¶…è¿‡é˜ˆå€¼
            width_ratio = (bbox[2] - bbox[0]) / page.rect.width
            height_ratio = (bbox[3] - bbox[1]) / page.rect.height
            
            if width_ratio < self.ocr_threshold[0] or height_ratio < self.ocr_threshold[1]:
                continue
            
            try:
                # æå–å›¾ç‰‡
                pix = fitz.Pixmap(doc, xref)
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                img_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(
                    pix.height, pix.width, -1
                )
                
                # å¦‚æœé¡µé¢æœ‰æ—‹è½¬ï¼Œæ—‹è½¬å›¾ç‰‡
                if int(page.rotation) != 0:
                    img_array = self._rotate_image(img_array, 360 - page.rotation)
                
                # OCRè¯†åˆ«
                result, _ = self.ocr_engine(img_array)
                if result:
                    ocr_result = [line[1] for line in result]
                    ocr_texts.extend(ocr_result)
                    
            except Exception as e:
                print(f"âš ï¸  OCRå¤„ç†å›¾ç‰‡å¤±è´¥ (é¡µ{page_num+1}): {e}")
                continue
        
        return "\n".join(ocr_texts)
    
    def _rotate_image(self, img: np.ndarray, angle: float) -> np.ndarray:
        """æ—‹è½¬å›¾ç‰‡"""
        import cv2
        
        h, w = img.shape[:2]
        center = (w / 2, h / 2)
        
        # è·å–æ—‹è½¬çŸ©é˜µ
        M = cv2.getRotationMatrix2D(center, angle, 1.0)
        
        # è®¡ç®—æ–°è¾¹ç•Œ
        new_w = int(h * np.abs(M[0, 1]) + w * np.abs(M[0, 0]))
        new_h = int(h * np.abs(M[0, 0]) + w * np.abs(M[0, 1]))
        
        # è°ƒæ•´æ—‹è½¬çŸ©é˜µ
        M[0, 2] += (new_w - w) / 2
        M[1, 2] += (new_h - h) / 2
        
        return cv2.warpAffine(img, M, (new_w, new_h))
    
    def _extract_txt(self, file_path: str) -> str:
        """æå–TXTæ–‡æœ¬"""
        encodings = ['utf-8', 'gbk', 'gb2312', 'utf-16']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    return self._clean_text(f.read())
            except UnicodeDecodeError:
                continue
        
        raise ValueError(f"æ— æ³•è§£ç æ–‡ä»¶: {file_path}")
    
    def _extract_json(self, file_path: str) -> str:
        """æå–JSONæ–‡æœ¬ï¼ˆæ”¯æŒQAæ ¼å¼ï¼‰"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        texts = []
        
        # æ”¯æŒå¤šç§JSONæ ¼å¼
        if isinstance(data, list):
            for item in data:
                if isinstance(item, dict):
                    # QAæ ¼å¼
                    if 'question' in item and 'answer' in item:
                        texts.append(f"é—®é¢˜ï¼š{item['question']}\nç­”æ¡ˆï¼š{item['answer']}")
                    # é€šç”¨æ ¼å¼
                    else:
                        texts.append(json.dumps(item, ensure_ascii=False))
                else:
                    texts.append(str(item))
        elif isinstance(data, dict):
            texts.append(json.dumps(data, ensure_ascii=False, indent=2))
        
        return self._clean_text("\n\n".join(texts))
    
    def _extract_csv(self, file_path: str) -> str:
        """æå–CSVæ–‡æœ¬ï¼ˆæ”¯æŒQAæ ¼å¼ï¼‰"""
        texts = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            for row in reader:
                # æ£€æŸ¥æ˜¯å¦æ˜¯QAæ ¼å¼
                if 'question' in row and 'answer' in row:
                    texts.append(f"é—®é¢˜ï¼š{row['question']}\nç­”æ¡ˆï¼š{row['answer']}")
                else:
                    # é€šç”¨æ ¼å¼ï¼šå°†æ‰€æœ‰åˆ—æ‹¼æ¥
                    row_text = " | ".join(f"{k}: {v}" for k, v in row.items() if v)
                    texts.append(row_text)
        
        return self._clean_text("\n\n".join(texts))
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        import re
        
        # å»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        
        # å»é™¤å¤šä½™æ¢è¡Œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # å»é™¤é¦–å°¾ç©ºç™½
        text = text.strip()
        
        return text
    
    def _split_text(
        self, 
        text: str, 
        filename: str, 
        file_type: str
    ) -> List[DocumentChunk]:
        """
        åˆ‡åˆ†æ–‡æœ¬
        
        ä½¿ç”¨æ»‘åŠ¨çª—å£æ–¹å¼åˆ‡åˆ†ï¼Œä¿è¯chunkä¹‹é—´æœ‰overlap
        """
        chunks = []
        
        if len(text) <= self.chunk_size:
            # æ–‡æœ¬å¤ªçŸ­ï¼Œä¸éœ€è¦åˆ‡åˆ†
            chunks.append(DocumentChunk(
                content=text,
                metadata={
                    "source": filename,
                    "file_type": file_type,
                    "chunk_index": 0,
                    "total_chunks": 1,
                    "text_length": len(text)
                },
                chunk_index=0
            ))
            return chunks
        
        # æ»‘åŠ¨çª—å£åˆ‡åˆ†
        start = 0
        chunk_index = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            # å°è¯•åœ¨å¥å­è¾¹ç•Œåˆ‡åˆ†
            if end < len(text):
                # å‘åæŸ¥æ‰¾å¥å­ç»“æŸç¬¦
                for sep in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', '\n', '.', '!', '?', ';']:
                    pos = text.rfind(sep, start, end)
                    if pos > start + self.chunk_size // 2:
                        end = pos + 1
                        break
            
            chunk_text = text[start:end].strip()
            
            if chunk_text:
                chunks.append(DocumentChunk(
                    content=chunk_text,
                    metadata={
                        "source": filename,
                        "file_type": file_type,
                        "chunk_index": chunk_index,
                        "text_length": len(chunk_text)
                    },
                    chunk_index=chunk_index
                ))
                chunk_index += 1
            
            # ä¸‹ä¸€ä¸ªçª—å£èµ·å§‹ä½ç½®ï¼ˆè€ƒè™‘overlapï¼‰
            start = end - self.chunk_overlap
            
            # é˜²æ­¢æ— é™å¾ªç¯
            if start >= len(text) - self.chunk_overlap:
                break
        
        # æ›´æ–°total_chunks
        for chunk in chunks:
            chunk.metadata["total_chunks"] = len(chunks)
        
        return chunks


class VectorIndexer:
    """å‘é‡ç´¢å¼•å™¨ - å°†æ–‡æ¡£å—å‘é‡åŒ–å¹¶å­˜å…¥ChromaDB"""
    
    def __init__(
        self,
        model_path: str = "data/models/AI-ModelScope/bge-large-zh-v1___5",
        chroma_path: str = "chroma_db",
        collection_name: str = "diabetes_knowledge",
        use_gpu: bool = True
    ):
        self.model_path = model_path
        self.chroma_path = chroma_path
        self.collection_name = collection_name
        self.use_gpu = use_gpu
        
        self.model = None
        self.chroma_client = None
        self.collection = None
    
    def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ®åº“"""
        import torch
        from sentence_transformers import SentenceTransformer
        import chromadb
        from chromadb.config import Settings
        
        # åŠ è½½å‘é‡æ¨¡å‹
        print("ğŸ”„ åŠ è½½å‘é‡æ¨¡å‹...")
        device = 'cuda' if self.use_gpu and torch.cuda.is_available() else 'cpu'
        self.model = SentenceTransformer(self.model_path, device=device)
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {device}")
        
        # åˆå§‹åŒ–ChromaDB
        print("ğŸ”„ åˆå§‹åŒ–ChromaDB...")
        self.chroma_client = chromadb.PersistentClient(
            path=self.chroma_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # è·å–æˆ–åˆ›å»ºcollection
        try:
            self.collection = self.chroma_client.get_collection(self.collection_name)
            print(f"âœ… å·²è¿æ¥åˆ°collection: {self.collection_name}")
        except:
            self.collection = self.chroma_client.create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            print(f"âœ… åˆ›å»ºæ–°collection: {self.collection_name}")
    
    def index_document(self, doc: ProcessedDocument) -> int:
        """
        å°†æ–‡æ¡£ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“
        
        Returns:
            int: ç´¢å¼•çš„chunkæ•°é‡
        """
        if not self.model or not self.collection:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ initialize() åˆå§‹åŒ–")
        
        print(f"ğŸ“¥ ç´¢å¼•æ–‡æ¡£: {doc.filename}")
        print(f"   åˆ†å—æ•°: {len(doc.chunks)}")
        
        # å‡†å¤‡æ•°æ®
        ids = []
        documents = []
        metadatas = []
        
        for chunk in doc.chunks:
            chunk_id = f"{doc.doc_id}_{chunk.chunk_index}"
            ids.append(chunk_id)
            documents.append(chunk.content)
            
            metadata = chunk.metadata.copy()
            metadata["doc_id"] = doc.doc_id
            metadata["filename"] = doc.filename
            metadata["created_at"] = doc.created_at
            metadatas.append(metadata)
        
        # æ‰¹é‡å‘é‡åŒ–
        print("ğŸ”„ å‘é‡åŒ–ä¸­...")
        embeddings = self.model.encode(
            documents,
            normalize_embeddings=True,
            show_progress_bar=False,
            batch_size=32
        )
        print(f"   âœ… å‘é‡åŒ–å®Œæˆï¼Œå…± {len(documents)} ä¸ªåˆ†å—")
        
        # å­˜å…¥ChromaDB
        print("ğŸ”„ å­˜å…¥æ•°æ®åº“...")
        self.collection.add(
            ids=ids,
            documents=documents,
            embeddings=embeddings.tolist(),
            metadatas=metadatas
        )
        
        print(f"âœ… ç´¢å¼•å®Œæˆï¼Œå…± {len(ids)} ä¸ªåˆ†å—")
        return len(ids)
    
    def get_stats(self) -> Dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡"""
        if not self.collection:
            return {"total_documents": 0}
        
        return {
            "total_documents": self.collection.count(),
            "collection_name": self.collection_name
        }


def process_and_index(
    file_path: str,
    chunk_size: int = 250,
    chunk_overlap: int = 50,
    use_ocr: bool = True
) -> Dict:
    """
    å¤„ç†å¹¶ç´¢å¼•å•ä¸ªæ–‡ä»¶çš„ä¾¿æ·å‡½æ•°
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        chunk_size: åˆ†å—å¤§å°
        chunk_overlap: åˆ†å—é‡å 
        use_ocr: æ˜¯å¦ä½¿ç”¨OCR
        
    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = DocumentProcessor(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        use_ocr=use_ocr
    )
    
    # å¤„ç†æ–‡æ¡£
    doc = processor.process_file(file_path)
    
    # åˆå§‹åŒ–ç´¢å¼•å™¨
    indexer = VectorIndexer()
    indexer.initialize()
    
    # ç´¢å¼•æ–‡æ¡£
    indexed_count = indexer.index_document(doc)
    
    return {
        "filename": doc.filename,
        "file_type": doc.file_type,
        "total_chars": doc.total_chars,
        "chunks": len(doc.chunks),
        "indexed": indexed_count,
        "doc_id": doc.doc_id
    }


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æ–‡æ¡£å¤„ç†å’Œå‘é‡åŒ–å·¥å…·")
    parser.add_argument("file", help="è¦å¤„ç†çš„æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--chunk-size", type=int, default=250, help="åˆ†å—å¤§å°")
    parser.add_argument("--chunk-overlap", type=int, default=50, help="åˆ†å—é‡å ")
    parser.add_argument("--no-ocr", action="store_true", help="ç¦ç”¨OCR")
    
    args = parser.parse_args()
    
    result = process_and_index(
        args.file,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
        use_ocr=not args.no_ocr
    )
    
    print("\n" + "=" * 50)
    print("å¤„ç†ç»“æœ:")
    print(f"  æ–‡ä»¶å: {result['filename']}")
    print(f"  ç±»å‹: {result['file_type']}")
    print(f"  æ€»å­—ç¬¦: {result['total_chars']}")
    print(f"  åˆ†å—æ•°: {result['chunks']}")
    print(f"  å·²ç´¢å¼•: {result['indexed']}")
    print(f"  æ–‡æ¡£ID: {result['doc_id']}")
