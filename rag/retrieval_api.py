#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç³–å°¿ç—…çŸ¥è¯†åº“RAGæ£€ç´¢APIæœåŠ¡
æä¾›é«˜æ€§èƒ½çš„è¯­ä¹‰æ£€ç´¢æœåŠ¡ï¼Œæ”¯æŒRedisç¼“å­˜å’Œæ¨¡å‹å¸¸é©»
"""

import os
import json
import time
import hashlib
import asyncio
from typing import List, Dict, Any, Optional, Union
from datetime import datetime, timedelta
from contextlib import asynccontextmanager

import uvicorn
import redis
import torch
import numpy as np
from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings

# é…ç½®ç®¡ç†
class Config:
    """æœåŠ¡é…ç½®"""
    # æ¨¡å‹é…ç½®
    MODEL_PATH = "data/models/AI-ModelScope/bge-large-zh-v1___5"
    USE_GPU = True
    
    # æ•°æ®åº“é…ç½®
    CHROMADB_PATH = "chroma_db"
    COLLECTION_NAME = "diabetes_knowledge"
    
    # Redisé…ç½®
    REDIS_HOST = "localhost"
    REDIS_PORT = 6379
    REDIS_DB = 2
    REDIS_PASSWORD = None
    CACHE_TTL = 1800  # 30åˆ†é’Ÿ
    
    # APIé…ç½®
    API_HOST = "0.0.0.0"
    API_PORT = 8001
    API_TITLE = "ç³–å°¿ç—…çŸ¥è¯†åº“RAGæ£€ç´¢API"
    API_VERSION = "1.0.0"
    
    # æ£€ç´¢é…ç½®
    DEFAULT_TOP_K = 5
    MAX_TOP_K = 20
    DEFAULT_SIMILARITY_THRESHOLD = 0.0
    ENABLE_CACHE = True
    
    # æ€§èƒ½é…ç½®
    MAX_QUERY_LENGTH = 500
    BATCH_SIZE = 32

# è¯·æ±‚å“åº”æ¨¡å‹
class SearchRequest(BaseModel):
    """æ£€ç´¢è¯·æ±‚æ¨¡å‹"""
    query: str = Field(..., description="æŸ¥è¯¢é—®é¢˜", max_length=Config.MAX_QUERY_LENGTH)
    top_k: int = Field(Config.DEFAULT_TOP_K, description="è¿”å›ç»“æœæ•°é‡", ge=1, le=Config.MAX_TOP_K)
    similarity_threshold: float = Field(Config.DEFAULT_SIMILARITY_THRESHOLD, description="ç›¸ä¼¼åº¦é˜ˆå€¼", ge=0.0, le=1.0)
    use_cache: bool = Field(True, description="æ˜¯å¦ä½¿ç”¨ç¼“å­˜")
    include_entities: bool = Field(True, description="æ˜¯å¦åŒ…å«åŒ»å­¦å®ä½“")
    category_filter: Optional[List[str]] = Field(None, description="åˆ†ç±»è¿‡æ»¤å™¨")

class SearchResult(BaseModel):
    """å•æ¡æ£€ç´¢ç»“æœ"""
    rank: int = Field(..., description="æ’å")
    question: str = Field(..., description="åŒ¹é…çš„é—®é¢˜")
    answer: str = Field(..., description="ç­”æ¡ˆå†…å®¹")
    category: str = Field(..., description="åˆ†ç±»")
    similarity: float = Field(..., description="ç›¸ä¼¼åº¦åˆ†æ•°")
    entities: Optional[List[str]] = Field(None, description="åŒ»å­¦å®ä½“")
    source_info: Optional[Dict] = Field(None, description="æ¥æºä¿¡æ¯")

class SearchResponse(BaseModel):
    """æ£€ç´¢å“åº”æ¨¡å‹"""
    success: bool = Field(..., description="æ˜¯å¦æˆåŠŸ")
    query: str = Field(..., description="æŸ¥è¯¢é—®é¢˜")
    results: List[SearchResult] = Field(..., description="æ£€ç´¢ç»“æœ")
    total_found: int = Field(..., description="æ‰¾åˆ°çš„æ€»æ•°")
    cache_hit: bool = Field(..., description="æ˜¯å¦å‘½ä¸­ç¼“å­˜")
    search_time: float = Field(..., description="æ£€ç´¢è€—æ—¶(ç§’)")
    timestamp: str = Field(..., description="å“åº”æ—¶é—´")

class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str
    model_loaded: bool
    database_connected: bool
    cache_connected: bool
    total_documents: int
    uptime: float
    gpu_available: bool
    memory_usage: Optional[Dict]

# æ¨¡å‹ç®¡ç†å™¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
class ModelManager:
    """BGEæ¨¡å‹ç®¡ç†å™¨ - å•ä¾‹æ¨¡å¼ç¡®ä¿æ¨¡å‹åªåŠ è½½ä¸€æ¬¡"""
    
    _instance = None
    _model = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ModelManager, cls).__new__(cls)
        return cls._instance
    
    async def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        if self._initialized:
            return
        
        print("ğŸ”„ åˆå§‹åŒ–BGEæ¨¡å‹...")
        try:
            device = 'cuda' if Config.USE_GPU and torch.cuda.is_available() else 'cpu'
            print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
            
            start_time = time.time()
            self._model = SentenceTransformer(Config.MODEL_PATH, device=device)
            load_time = time.time() - start_time
            
            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ! ç”¨æ—¶: {load_time:.2f}ç§’")
            print(f"ğŸ“Š å‘é‡ç»´åº¦: {self._model.get_sentence_embedding_dimension()}")
            
            if Config.USE_GPU and torch.cuda.is_available():
                memory_allocated = torch.cuda.memory_allocated() / (1024**2)
                print(f"ğŸ’¾ GPUå†…å­˜ä½¿ç”¨: {memory_allocated:.1f}MB")
            
            self._initialized = True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def encode(self, texts: Union[str, List[str]], **kwargs) -> np.ndarray:
        """ç¼–ç æ–‡æœ¬"""
        if not self._initialized:
            raise RuntimeError("æ¨¡å‹æœªåˆå§‹åŒ–")
        
        if isinstance(texts, str):
            texts = [texts]
        
        return self._model.encode(
            texts, 
            normalize_embeddings=True,
            show_progress_bar=False,
            **kwargs
        )
    
    @property
    def is_initialized(self) -> bool:
        return self._initialized

# ç¼“å­˜ç®¡ç†å™¨
class CacheManager:
    """Redisç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self.redis_client = None
        self.connected = False
    
    async def initialize(self):
        """åˆå§‹åŒ–Redisè¿æ¥"""
        try:
            self.redis_client = redis.Redis(
                host=Config.REDIS_HOST,
                port=Config.REDIS_PORT,
                db=Config.REDIS_DB,
                password=Config.REDIS_PASSWORD,
                decode_responses=True,
                socket_timeout=5,
                socket_connect_timeout=5
            )
            
            # æµ‹è¯•è¿æ¥
            self.redis_client.ping()
            self.connected = True
            print("âœ… Redisè¿æ¥æˆåŠŸ")
            
        except Exception as e:
            print(f"âš ï¸  Redisè¿æ¥å¤±è´¥: {e}")
            self.connected = False
    
    def generate_cache_key(self, query: str, top_k: int, threshold: float, filters: Dict = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_data = {
            "query": query.strip().lower(),
            "top_k": top_k,
            "threshold": threshold,
            "filters": filters or {}
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return f"rag_query:{hashlib.md5(cache_str.encode()).hexdigest()}"
    
    async def get_cached_result(self, cache_key: str) -> Optional[Dict]:
        """è·å–ç¼“å­˜ç»“æœ"""
        if not self.connected or not Config.ENABLE_CACHE:
            return None
        
        try:
            cached_data = self.redis_client.get(cache_key)
            if cached_data:
                return json.loads(cached_data)
        except Exception as e:
            print(f"ç¼“å­˜è¯»å–é”™è¯¯: {e}")
        
        return None
    
    async def set_cache_result(self, cache_key: str, result: Dict):
        """è®¾ç½®ç¼“å­˜ç»“æœ"""
        if not self.connected or not Config.ENABLE_CACHE:
            return
        
        try:
            self.redis_client.setex(
                cache_key,
                Config.CACHE_TTL,
                json.dumps(result, ensure_ascii=False)
            )
        except Exception as e:
            print(f"ç¼“å­˜å†™å…¥é”™è¯¯: {e}")
    
    async def clear_cache(self, pattern: str = "rag_query:*"):
        """æ¸…ç†ç¼“å­˜"""
        if not self.connected:
            return 0
        
        try:
            keys = self.redis_client.keys(pattern)
            if keys:
                return self.redis_client.delete(*keys)
            return 0
        except Exception as e:
            print(f"ç¼“å­˜æ¸…ç†é”™è¯¯: {e}")
            return 0

# æ£€ç´¢æœåŠ¡
class RetrievalService:
    """æ£€ç´¢æœåŠ¡æ ¸å¿ƒç±»"""
    
    def __init__(self):
        self.model_manager = ModelManager()
        self.cache_manager = CacheManager()
        self.chroma_client = None
        self.collection = None
        self.total_documents = 0
        self.start_time = time.time()
    
    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        # åˆå§‹åŒ–æ¨¡å‹
        await self.model_manager.initialize()
        
        # åˆå§‹åŒ–ç¼“å­˜
        await self.cache_manager.initialize()
        
        # åˆå§‹åŒ–ChromaDB
        await self.refresh_collection()
    
    async def refresh_collection(self):
        """åˆ·æ–°ChromaDBè¿æ¥"""
        try:
            self.chroma_client = chromadb.PersistentClient(
                path=Config.CHROMADB_PATH,
                settings=Settings(anonymized_telemetry=False)
            )
            
            # å°è¯•è·å–ç°æœ‰collectionï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
            try:
                self.collection = self.chroma_client.get_collection(Config.COLLECTION_NAME)
            except:
                self.collection = self.chroma_client.create_collection(
                    name=Config.COLLECTION_NAME,
                    metadata={"hnsw:space": "cosine"}
                )
            
            self.total_documents = self.collection.count()
            print(f"âœ… ChromaDBè¿æ¥æˆåŠŸï¼Œæ€»æ–‡æ¡£æ•°: {self.total_documents}")
            
        except Exception as e:
            print(f"âŒ ChromaDBè¿æ¥å¤±è´¥: {e}")
            raise
    
    async def search(self, request: SearchRequest) -> SearchResponse:
        """æ‰§è¡Œæ£€ç´¢"""
        start_time = time.time()
        cache_hit = False
        
        # æŸ¥è¯¢é¢„å¤„ç†
        query = request.query.strip()
        if not query:
            raise HTTPException(status_code=400, detail="æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self.cache_manager.generate_cache_key(
            query, request.top_k, request.similarity_threshold, 
            {"category_filter": request.category_filter}
        )
        
        # å°è¯•ä»ç¼“å­˜è·å–
        if request.use_cache and Config.ENABLE_CACHE:
            cached_result = await self.cache_manager.get_cached_result(cache_key)
            if cached_result:
                cached_result["cache_hit"] = True
                cached_result["search_time"] = time.time() - start_time
                cached_result["timestamp"] = datetime.now().isoformat()
                return SearchResponse(**cached_result)
        
        try:
            # å‘é‡åŒ–æŸ¥è¯¢
            query_embedding = self.model_manager.encode([query])
            
            # æ‰§è¡Œæ£€ç´¢
            chroma_results = self.collection.query(
                query_embeddings=query_embedding.tolist(),
                n_results=min(request.top_k * 2, Config.MAX_TOP_K),  # è·å–æ›´å¤šç»“æœç”¨äºè¿‡æ»¤
                include=['documents', 'metadatas', 'distances']
            )
            
            # å¤„ç†ç»“æœ
            results = await self._process_results(
                chroma_results, request, query_embedding
            )
            
            # æ„å»ºå“åº”
            response_data = {
                "success": True,
                "query": query,
                "results": results[:request.top_k],  # é™åˆ¶æœ€ç»ˆè¿”å›æ•°é‡
                "total_found": len(results),
                "cache_hit": cache_hit,
                "search_time": time.time() - start_time,
                "timestamp": datetime.now().isoformat()
            }
            
            # ç¼“å­˜ç»“æœ
            if request.use_cache and Config.ENABLE_CACHE:
                cache_data = response_data.copy()
                cache_data.pop("cache_hit")
                cache_data.pop("search_time") 
                cache_data.pop("timestamp")
                await self.cache_manager.set_cache_result(cache_key, cache_data)
            
            return SearchResponse(**response_data)
            
        except Exception as e:
            print(f"æ£€ç´¢é”™è¯¯: {e}")
            raise HTTPException(status_code=500, detail=f"æ£€ç´¢å¤±è´¥: {str(e)}")
    
    async def _process_results(self, chroma_results: Dict, request: SearchRequest, query_embedding: np.ndarray) -> List[Dict]:
        """å¤„ç†å’Œä¼˜åŒ–æ£€ç´¢ç»“æœ"""
        if not chroma_results['documents'] or not chroma_results['documents'][0]:
            return []
        
        results = []
        
        for i, (doc, metadata, distance) in enumerate(zip(
            chroma_results['documents'][0],
            chroma_results['metadatas'][0],
            chroma_results['distances'][0]
        )):
            similarity = 1 - distance
            
            # åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼
            if similarity < request.similarity_threshold:
                continue
            
            # åº”ç”¨åˆ†ç±»è¿‡æ»¤
            if (request.category_filter and 
                metadata.get('category') not in request.category_filter):
                continue
            
            # è§£æå®ä½“
            entities = []
            if request.include_entities:
                try:
                    entities = json.loads(metadata.get('entities', '[]'))
                except:
                    entities = []
            
            result = {
                "rank": len(results) + 1,
                "question": metadata.get('question', ''),
                "answer": doc,
                "category": metadata.get('category', ''),
                "similarity": round(similarity, 4),
                "entities": entities if request.include_entities else None,
                "source_info": {
                    "source_row": metadata.get('source_row'),
                    "chunk_index": metadata.get('chunk_index'),
                    "text_length": metadata.get('text_length'),
                    "doc_id": metadata.get('doc_id', 'other'),
                    "source": metadata.get('source', metadata.get('filename', 'æœªçŸ¥æ¥æº'))
                }
            }
            
            results.append(result)
        
        # ç»“æœæ’åºå’Œå»é‡
        results = await self._deduplicate_results(results)
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        return results
    
    async def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """å»é‡å¤„ç†"""
        seen_answers = set()
        unique_results = []
        
        for result in results:
            answer_hash = hashlib.md5(result['answer'].encode()).hexdigest()
            if answer_hash not in seen_answers:
                seen_answers.add(answer_hash)
                unique_results.append(result)
        
        return unique_results
    
    def get_health_status(self) -> HealthResponse:
        """è·å–å¥åº·çŠ¶æ€"""
        memory_info = None
        if torch.cuda.is_available():
            memory_info = {
                "gpu_memory_allocated": f"{torch.cuda.memory_allocated() / (1024**2):.1f}MB",
                "gpu_memory_cached": f"{torch.cuda.memory_reserved() / (1024**2):.1f}MB"
            }
        
        # æ ¸å¿ƒæœåŠ¡ï¼šæ¨¡å‹å’Œæ•°æ®åº“ï¼ŒRedisæ˜¯å¯é€‰çš„
        is_healthy = self.model_manager.is_initialized and self.collection is not None
        
        return HealthResponse(
            status="healthy" if is_healthy else "unhealthy",
            model_loaded=self.model_manager.is_initialized,
            database_connected=self.collection is not None,
            cache_connected=self.cache_manager.connected,
            total_documents=self.total_documents,
            uptime=time.time() - self.start_time,
            gpu_available=torch.cuda.is_available(),
            memory_usage=memory_info
        )

# å…¨å±€æœåŠ¡å®ä¾‹
retrieval_service = RetrievalService()

# FastAPIç”Ÿå‘½å‘¨æœŸç®¡ç†
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    print("ğŸš€ å¯åŠ¨æ£€ç´¢APIæœåŠ¡...")
    
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    try:
        await retrieval_service.initialize()
        print("âœ… æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âŒ æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
    
    yield  # åº”ç”¨è¿è¡Œ
    
    # å…³é—­æ—¶æ¸…ç†
    print("ğŸ”„ å…³é—­æ£€ç´¢APIæœåŠ¡...")
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    print("âœ… æœåŠ¡å…³é—­å®Œæˆ")

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title=Config.API_TITLE,
    version=Config.API_VERSION,
    description="åŸºäºBGEæ¨¡å‹å’ŒChromaDBçš„ç³–å°¿ç—…çŸ¥è¯†åº“è¯­ä¹‰æ£€ç´¢APIæœåŠ¡",
    lifespan=lifespan
)

# æ·»åŠ CORSæ”¯æŒ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# APIè·¯ç”±
@app.get("/", summary="æœåŠ¡ä¿¡æ¯")
async def root():
    """è·å–æœåŠ¡åŸºæœ¬ä¿¡æ¯"""
    return {
        "service": Config.API_TITLE,
        "version": Config.API_VERSION,
        "status": "running",
        "docs": "/docs",
        "health": "/health"
    }

@app.get("/health", response_model=HealthResponse, summary="å¥åº·æ£€æŸ¥")
async def health_check():
    """è·å–æœåŠ¡å¥åº·çŠ¶æ€"""
    return retrieval_service.get_health_status()

@app.post("/search", response_model=SearchResponse, summary="è¯­ä¹‰æ£€ç´¢")
async def search_knowledge(request: SearchRequest):
    """
    æ‰§è¡Œè¯­ä¹‰æ£€ç´¢
    
    - **query**: æŸ¥è¯¢é—®é¢˜
    - **top_k**: è¿”å›ç»“æœæ•°é‡ (1-20)
    - **similarity_threshold**: ç›¸ä¼¼åº¦é˜ˆå€¼ (0.0-1.0)
    - **use_cache**: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
    - **include_entities**: æ˜¯å¦åŒ…å«åŒ»å­¦å®ä½“
    - **category_filter**: åˆ†ç±»è¿‡æ»¤å™¨
    """
    return await retrieval_service.search(request)

@app.post("/cache/clear", summary="æ¸…ç†ç¼“å­˜")
async def clear_cache():
    """æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
    cleared_count = await retrieval_service.cache_manager.clear_cache()
    return {
        "success": True,
        "message": f"å·²æ¸…ç† {cleared_count} ä¸ªç¼“å­˜é¡¹",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/stats", summary="æœåŠ¡ç»Ÿè®¡")
async def get_stats():
    """è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
    health = retrieval_service.get_health_status()
    
    return {
        "service_status": health.status,
        "uptime": health.uptime,
        "total_documents": health.total_documents,
        "model_loaded": health.model_loaded,
        "gpu_available": health.gpu_available,
        "cache_connected": health.cache_connected,
        "config": {
            "max_top_k": Config.MAX_TOP_K,
            "cache_ttl": Config.CACHE_TTL,
            "enable_cache": Config.ENABLE_CACHE
        }
    }

# ============ MCP æŸ¥è¯¢æ¥å£ ============
class MCPQueryRequest(BaseModel):
    """MCPæŸ¥è¯¢è¯·æ±‚ - ç®€åŒ–ç‰ˆæœ¬ä¾›AI Agentè°ƒç”¨"""
    query: str = Field(..., description="æŸ¥è¯¢é—®é¢˜")
    top_k: int = Field(3, description="è¿”å›ç»“æœæ•°é‡", ge=1, le=10)

@app.post("/mcp/query", summary="MCPçŸ¥è¯†åº“æŸ¥è¯¢æ¥å£")
async def mcp_query(request: MCPQueryRequest):
    """
    MCPå·¥å…·è°ƒç”¨æ¥å£ - ä¾›AI AgentæŸ¥è¯¢ç³–å°¿ç—…çŸ¥è¯†åº“
    
    è¿”å›æ ¼å¼åŒ–çš„çŸ¥è¯†åº“æ£€ç´¢ç»“æœï¼Œé€‚åˆAIç†è§£å’Œä½¿ç”¨
    """
    try:
        search_request = SearchRequest(
            query=request.query,
            top_k=request.top_k,
            similarity_threshold=0.3,
            use_cache=True,
            include_entities=True
        )
        result = await retrieval_service.search(search_request)
        
        # æ ¼å¼åŒ–ä¸ºMCPå‹å¥½çš„å“åº”
        knowledge_items = []
        for item in result.results:
            knowledge_items.append({
                "question": item.question,
                "answer": item.answer,
                "category": item.category,
                "relevance": f"{item.similarity * 100:.1f}%",
                "entities": item.entities or []
            })
        
        return {
            "success": True,
            "query": request.query,
            "knowledge_count": len(knowledge_items),
            "knowledge": knowledge_items,
            "summary": f"æ‰¾åˆ° {len(knowledge_items)} æ¡ç›¸å…³çŸ¥è¯†" if knowledge_items else "æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†"
        }
    except Exception as e:
        return {
            "success": False,
            "query": request.query,
            "knowledge_count": 0,
            "knowledge": [],
            "error": str(e)
        }

# ============ æ–‡æ¡£ç®¡ç†æ¥å£ ============
import shutil
from fastapi import UploadFile, File
from pathlib import Path

# æ–‡æ¡£å­˜å‚¨ç›®å½•
UPLOAD_DIR = Path("data/uploads")
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

# æ–‡æ¡£å…ƒæ•°æ®å­˜å‚¨ï¼ˆå†…å­˜ç¼“å­˜ï¼Œå¯åŠ¨æ—¶ä»ChromaDBæ¢å¤ï¼‰
documents_db: Dict[str, Dict] = {}


def init_documents_db():
    """ä»ChromaDBæ¢å¤æ–‡æ¡£åˆ—è¡¨"""
    global documents_db
    try:
        if retrieval_service.collection:
            # è·å–æ‰€æœ‰å”¯ä¸€çš„doc_id
            results = retrieval_service.collection.get(include=["metadatas"])
            if results and results["metadatas"]:
                doc_map = {}
                for metadata in results["metadatas"]:
                    doc_id = metadata.get("doc_id", "other")
                    if doc_id not in doc_map:
                        doc_map[doc_id] = {
                            "id": doc_id,
                            "name": metadata.get("source", metadata.get("filename", "æœªçŸ¥æ–‡æ¡£")),
                            "type": metadata.get("file_type", "unknown"),
                            "chunks": 0,
                            "size": 0,
                            "created_at": metadata.get("created_at", "æœªçŸ¥")
                        }
                    doc_map[doc_id]["chunks"] += 1
                
                documents_db = doc_map
                print(f"âœ… ä»ChromaDBæ¢å¤äº† {len(documents_db)} ä¸ªæ–‡æ¡£è®°å½•")
    except Exception as e:
        print(f"âš ï¸  æ¢å¤æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")


@app.get("/documents", summary="è·å–æ–‡æ¡£åˆ—è¡¨")
async def get_documents():
    """è·å–å·²å¯¼å…¥çš„æ–‡æ¡£åˆ—è¡¨"""
    try:
        # å¦‚æœå†…å­˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»ChromaDBæ¢å¤
        if not documents_db:
            init_documents_db()
        
        docs_list = list(documents_db.values())
        
        # å¦‚æœè¿˜æ˜¯ç©ºçš„ï¼Œè¯´æ˜æ•°æ®åº“é‡Œä¹Ÿæ²¡æœ‰æŒ‰doc_idç»„ç»‡çš„æ•°æ®ï¼Œè¿”å›ä¸€ä¸ª"å…¶ä»–"åˆ†ç±»
        if not docs_list:
            total = retrieval_service.total_documents
            if total > 0:
                docs_list = [{
                    "id": "other",
                    "name": "åŸæœ‰çŸ¥è¯†åº“æ•°æ®",
                    "type": "csv",
                    "chunks": total,
                    "size": 0,
                    "created_at": "å¯¼å…¥æ—¶é—´æœªçŸ¥"
                }]
        
        return {"success": True, "documents": docs_list}
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"success": False, "documents": [], "error": str(e)}

@app.post("/documents/upload", summary="ä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£")
async def upload_document(file: UploadFile = File(...)):
    """
    ä¸Šä¼ æ–‡æ¡£å¹¶è¿›è¡Œå‘é‡åŒ–å¤„ç†
    
    æ”¯æŒæ ¼å¼: PDF, TXT, JSON, CSV
    å¤„ç†æµç¨‹: æå–æ–‡æœ¬ -> åˆ†å—(250å­—ç¬¦,50é‡å ) -> å‘é‡åŒ– -> å­˜å…¥ChromaDB
    """
    try:
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        allowed_types = {'.pdf', '.txt', '.json', '.csv'}
        file_ext = Path(file.filename).suffix.lower()
        if file_ext not in allowed_types:
            return {"success": False, "error": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}"}
        
        # ä¿å­˜ä¸Šä¼ æ–‡ä»¶
        print(f"ğŸ“¥ ä¿å­˜æ–‡ä»¶: {file.filename}")
        file_path = UPLOAD_DIR / file.filename
        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)
        print(f"   æ–‡ä»¶å¤§å°: {len(content)} bytes")
        
        # å¯¼å…¥æ–‡æ¡£å¤„ç†å™¨
        from document_processor import DocumentProcessor, VectorIndexer
        
        # å¤„ç†æ–‡æ¡£
        print("ğŸ”„ å¼€å§‹å¤„ç†æ–‡æ¡£...")
        processor = DocumentProcessor(
            chunk_size=250,
            chunk_overlap=50,
            use_ocr=(file_ext == '.pdf')
        )
        doc = processor.process_file(str(file_path))
        print(f"   æ–‡æ¡£åˆ†å—æ•°: {len(doc.chunks)}")
        
        # å‘é‡åŒ–å¹¶ç´¢å¼•
        print("ğŸ”„ å¼€å§‹å‘é‡åŒ–ç´¢å¼•...")
        indexer = VectorIndexer(
            model_path=Config.MODEL_PATH,
            chroma_path=Config.CHROMADB_PATH,
            collection_name=Config.COLLECTION_NAME
        )
        indexer.initialize()
        indexed_count = indexer.index_document(doc)
        
        # åˆ·æ–°æ£€ç´¢æœåŠ¡çš„collectionè¿æ¥
        await retrieval_service.refresh_collection()
        
        # è®°å½•æ–‡æ¡£å…ƒæ•°æ®
        doc_info = {
            "id": doc.doc_id,
            "name": doc.filename,
            "type": doc.file_type,
            "chunks": len(doc.chunks),
            "size": len(content),
            "created_at": doc.created_at
        }
        documents_db[doc.doc_id] = doc_info
        
        print(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {doc.filename}, ç´¢å¼• {indexed_count} ä¸ªåˆ†å—")
        
        return {
            "success": True,
            "message": f"æ–‡æ¡£å¤„ç†å®Œæˆï¼Œå·²ç´¢å¼• {indexed_count} ä¸ªåˆ†å—",
            "document": doc_info
        }
        
    except Exception as e:
        import traceback
        print(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
        traceback.print_exc()
        return {"success": False, "error": str(e)}

@app.delete("/documents/{doc_id}", summary="åˆ é™¤æ–‡æ¡£")
async def delete_document(doc_id: str):
    """åˆ é™¤æŒ‡å®šæ–‡æ¡£ï¼ˆä»ç´¢å¼•ä¸­ç§»é™¤ï¼‰"""
    try:
        if doc_id in documents_db:
            del documents_db[doc_id]
        
        # æ³¨æ„ï¼šChromaDBåˆ é™¤éœ€è¦çŸ¥é“æ‰€æœ‰chunkçš„ID
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¹æ®doc_idå‰ç¼€åˆ é™¤
        return {"success": True, "message": "æ–‡æ¡£å·²åˆ é™¤"}
    except Exception as e:
        return {"success": False, "error": str(e)}

@app.post("/index/rebuild", summary="é‡å»ºç´¢å¼•")
async def rebuild_index():
    """é‡å»ºå‘é‡ç´¢å¼•"""
    try:
        # é‡æ–°åˆå§‹åŒ–collection
        import chromadb
        from chromadb.config import Settings
        
        client = chromadb.PersistentClient(
            path=Config.CHROMADB_PATH,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # åˆ é™¤å¹¶é‡å»ºcollection
        try:
            client.delete_collection(Config.COLLECTION_NAME)
        except:
            pass
        
        client.create_collection(
            name=Config.COLLECTION_NAME,
            metadata={"hnsw:space": "cosine"}
        )
        
        # é‡æ–°å¤„ç†æ‰€æœ‰ä¸Šä¼ çš„æ–‡æ¡£
        from document_processor import DocumentProcessor, VectorIndexer
        
        processor = DocumentProcessor(chunk_size=250, chunk_overlap=50)
        indexer = VectorIndexer(
            model_path=Config.MODEL_PATH,
            chroma_path=Config.CHROMADB_PATH,
            collection_name=Config.COLLECTION_NAME
        )
        indexer.initialize()
        
        total_indexed = 0
        for file_path in UPLOAD_DIR.glob("*"):
            if file_path.suffix.lower() in {'.pdf', '.txt', '.json', '.csv'}:
                doc = processor.process_file(str(file_path))
                total_indexed += indexer.index_document(doc)
        
        retrieval_service.total_documents = total_indexed
        
        return {
            "success": True,
            "message": f"ç´¢å¼•é‡å»ºå®Œæˆï¼Œå…±ç´¢å¼• {total_indexed} ä¸ªåˆ†å—"
        }
    except Exception as e:
        return {"success": False, "error": str(e)}


# ============ çŸ¥è¯†æ¡ç›®ç®¡ç†æ¥å£ ============
class KnowledgeItemCreate(BaseModel):
    """åˆ›å»ºçŸ¥è¯†æ¡ç›®"""
    content: str = Field(..., description="çŸ¥è¯†å†…å®¹")
    question: Optional[str] = Field(None, description="ç›¸å…³é—®é¢˜")
    category: Optional[str] = Field("å…¶ä»–", description="åˆ†ç±»")
    doc_id: Optional[str] = Field("other", description="æ‰€å±æ–‡æ¡£ID")

class KnowledgeItemUpdate(BaseModel):
    """æ›´æ–°çŸ¥è¯†æ¡ç›®"""
    content: Optional[str] = Field(None, description="çŸ¥è¯†å†…å®¹")
    question: Optional[str] = Field(None, description="ç›¸å…³é—®é¢˜")
    category: Optional[str] = Field(None, description="åˆ†ç±»")

@app.get("/knowledge/items", summary="è·å–çŸ¥è¯†æ¡ç›®åˆ—è¡¨ï¼ˆåˆ†é¡µï¼‰")
async def get_knowledge_items(
    doc_id: Optional[str] = None,
    page: int = 1,
    page_size: int = 20
):
    """
    è·å–çŸ¥è¯†æ¡ç›®åˆ—è¡¨ï¼Œæ”¯æŒæŒ‰æ–‡æ¡£ç­›é€‰å’Œåˆ†é¡µ
    
    - doc_id: æ–‡æ¡£IDï¼Œä¸ä¼ åˆ™è·å–æ‰€æœ‰ï¼Œä¼ "other"è·å–æ— å½’å±çš„æ¡ç›®
    - page: é¡µç ï¼Œä»1å¼€å§‹
    - page_size: æ¯é¡µæ•°é‡
    """
    try:
        collection = retrieval_service.collection
        if not collection:
            return {"success": False, "error": "æ•°æ®åº“æœªè¿æ¥"}
        
        offset = (page - 1) * page_size
        
        # è·å–æ‰€æœ‰æ•°æ®ï¼ˆChromaDBçš„whereæŸ¥è¯¢å¯¹ç¼ºå¤±å­—æ®µæ”¯æŒä¸å¥½ï¼‰
        all_results = collection.get(
            include=["documents", "metadatas"],
            limit=10000  # è·å–è¶³å¤Ÿå¤šçš„æ•°æ®
        )
        
        # åœ¨å†…å­˜ä¸­ç­›é€‰
        filtered_items = []
        if all_results and all_results['ids']:
            for i, item_id in enumerate(all_results['ids']):
                metadata = all_results['metadatas'][i] if all_results['metadatas'] else {}
                item_doc_id = metadata.get("doc_id", "other")
                
                # å¦‚æœæŒ‡å®šäº†doc_idï¼Œè¿›è¡Œç­›é€‰
                if doc_id:
                    if doc_id == "other":
                        # æŸ¥æ‰¾æ²¡æœ‰doc_idæˆ–doc_idä¸ºotherçš„æ¡ç›®
                        if item_doc_id not in ["other", None, ""] and "doc_id" in metadata:
                            continue
                    else:
                        if item_doc_id != doc_id:
                            continue
                
                filtered_items.append({
                    "id": item_id,
                    "content": all_results['documents'][i] if all_results['documents'] else "",
                    "question": metadata.get("question", ""),
                    "category": metadata.get("category", "å…¶ä»–"),
                    "doc_id": item_doc_id,
                    "source": metadata.get("source", metadata.get("filename", "æœªçŸ¥")),
                    "chunk_index": metadata.get("chunk_index", 0)
                })
        
        # åˆ†é¡µ
        total_count = len(filtered_items)
        paginated_items = filtered_items[offset:offset + page_size]
        
        return {
            "success": True,
            "items": paginated_items,
            "total": total_count,
            "page": page,
            "page_size": page_size,
            "total_pages": (total_count + page_size - 1) // page_size if total_count > 0 else 1
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e), "items": [], "total": 0}

@app.get("/knowledge/items/{item_id}", summary="è·å–å•ä¸ªçŸ¥è¯†æ¡ç›®")
async def get_knowledge_item(item_id: str):
    """è·å–å•ä¸ªçŸ¥è¯†æ¡ç›®è¯¦æƒ…"""
    try:
        collection = retrieval_service.collection
        result = collection.get(ids=[item_id], include=["documents", "metadatas", "embeddings"])
        
        if not result['ids']:
            return {"success": False, "error": "æ¡ç›®ä¸å­˜åœ¨"}
        
        metadata = result['metadatas'][0] if result['metadatas'] else {}
        return {
            "success": True,
            "item": {
                "id": item_id,
                "content": result['documents'][0] if result['documents'] else "",
                "question": metadata.get("question", ""),
                "category": metadata.get("category", "å…¶ä»–"),
                "doc_id": metadata.get("doc_id", "other"),
                "source": metadata.get("source", metadata.get("filename", "æœªçŸ¥")),
                "chunk_index": metadata.get("chunk_index", 0)
            }
        }
    except Exception as e:
        return {"success": False, "error": str(e)}

@app.post("/knowledge/items", summary="åˆ›å»ºçŸ¥è¯†æ¡ç›®")
async def create_knowledge_item(item: KnowledgeItemCreate):
    """æ‰‹åŠ¨åˆ›å»ºä¸€ä¸ªçŸ¥è¯†æ¡ç›®"""
    try:
        collection = retrieval_service.collection
        
        # ç”ŸæˆID
        item_id = hashlib.md5(f"{item.content}_{datetime.now().isoformat()}".encode()).hexdigest()[:16]
        
        # å‘é‡åŒ–
        embedding = retrieval_service.model_manager.encode([item.content])[0]
        
        # å­˜å…¥æ•°æ®åº“
        collection.add(
            ids=[item_id],
            documents=[item.content],
            embeddings=[embedding.tolist()],
            metadatas=[{
                "question": item.question or "",
                "category": item.category or "å…¶ä»–",
                "doc_id": item.doc_id or "other",
                "source": "æ‰‹åŠ¨æ·»åŠ ",
                "created_at": datetime.now().isoformat()
            }]
        )
        
        retrieval_service.total_documents = collection.count()
        
        return {
            "success": True,
            "message": "çŸ¥è¯†æ¡ç›®åˆ›å»ºæˆåŠŸ",
            "item_id": item_id
        }
    except Exception as e:
        return {"success": False, "error": str(e)}

@app.put("/knowledge/items/{item_id}", summary="æ›´æ–°çŸ¥è¯†æ¡ç›®")
async def update_knowledge_item(item_id: str, item: KnowledgeItemUpdate):
    """æ›´æ–°çŸ¥è¯†æ¡ç›®å†…å®¹"""
    try:
        collection = retrieval_service.collection
        
        # è·å–åŸæ•°æ®
        result = collection.get(ids=[item_id], include=["documents", "metadatas"])
        if not result['ids']:
            return {"success": False, "error": "æ¡ç›®ä¸å­˜åœ¨"}
        
        old_content = result['documents'][0]
        old_metadata = result['metadatas'][0] if result['metadatas'] else {}
        
        # æ›´æ–°å†…å®¹
        new_content = item.content if item.content else old_content
        new_metadata = old_metadata.copy()
        if item.question is not None:
            new_metadata["question"] = item.question
        if item.category is not None:
            new_metadata["category"] = item.category
        new_metadata["updated_at"] = datetime.now().isoformat()
        
        # å¦‚æœå†…å®¹å˜äº†ï¼Œéœ€è¦é‡æ–°å‘é‡åŒ–
        if item.content and item.content != old_content:
            new_embedding = retrieval_service.model_manager.encode([new_content])[0]
            collection.update(
                ids=[item_id],
                documents=[new_content],
                embeddings=[new_embedding.tolist()],
                metadatas=[new_metadata]
            )
        else:
            collection.update(
                ids=[item_id],
                metadatas=[new_metadata]
            )
        
        return {"success": True, "message": "æ›´æ–°æˆåŠŸ"}
    except Exception as e:
        return {"success": False, "error": str(e)}

@app.delete("/knowledge/items/{item_id}", summary="åˆ é™¤çŸ¥è¯†æ¡ç›®")
async def delete_knowledge_item(item_id: str):
    """åˆ é™¤å•ä¸ªçŸ¥è¯†æ¡ç›®"""
    try:
        collection = retrieval_service.collection
        collection.delete(ids=[item_id])
        retrieval_service.total_documents = collection.count()
        return {"success": True, "message": "åˆ é™¤æˆåŠŸ"}
    except Exception as e:
        return {"success": False, "error": str(e)}


class BatchDeleteRequest(BaseModel):
    """æ‰¹é‡åˆ é™¤è¯·æ±‚"""
    item_ids: List[str] = Field(..., description="è¦åˆ é™¤çš„çŸ¥è¯†æ¡ç›®IDåˆ—è¡¨")


@app.post("/knowledge/items/batch-delete", summary="æ‰¹é‡åˆ é™¤çŸ¥è¯†æ¡ç›®")
async def batch_delete_knowledge_items(request: BatchDeleteRequest):
    """æ‰¹é‡åˆ é™¤å¤šä¸ªçŸ¥è¯†æ¡ç›®"""
    try:
        if not request.item_ids:
            return {"success": False, "error": "è¯·æä¾›è¦åˆ é™¤çš„æ¡ç›®ID"}
        
        collection = retrieval_service.collection
        
        # æ‰¹é‡åˆ é™¤
        collection.delete(ids=request.item_ids)
        deleted_count = len(request.item_ids)
        
        retrieval_service.total_documents = collection.count()
        
        return {
            "success": True,
            "message": f"æˆåŠŸåˆ é™¤ {deleted_count} ä¸ªçŸ¥è¯†æ¡ç›®",
            "deleted_count": deleted_count
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

@app.delete("/documents/{doc_id}/all", summary="åˆ é™¤æ–‡æ¡£åŠå…¶æ‰€æœ‰çŸ¥è¯†æ¡ç›®")
async def delete_document_with_items(doc_id: str):
    """åˆ é™¤æ–‡æ¡£åŠå…¶ä¸‹æ‰€æœ‰çŸ¥è¯†æ¡ç›®"""
    try:
        collection = retrieval_service.collection
        
        # æŸ¥æ‰¾è¯¥æ–‡æ¡£ä¸‹æ‰€æœ‰æ¡ç›®çš„ID
        # ChromaDB where æŸ¥è¯¢
        results = collection.get(
            where={"doc_id": doc_id},
            include=["metadatas"]
        )
        
        if results['ids']:
            # æ‰¹é‡åˆ é™¤
            collection.delete(ids=results['ids'])
            deleted_count = len(results['ids'])
        else:
            deleted_count = 0
        
        # åˆ é™¤æ–‡æ¡£è®°å½•
        if doc_id in documents_db:
            del documents_db[doc_id]
        
        # åˆ é™¤ä¸Šä¼ çš„æ–‡ä»¶
        for file_path in UPLOAD_DIR.glob("*"):
            # ç®€å•åŒ¹é…ï¼Œå®é™…åº”è¯¥å­˜å‚¨æ–‡ä»¶åå’Œdoc_idçš„æ˜ å°„
            pass
        
        retrieval_service.total_documents = collection.count()
        
        return {
            "success": True,
            "message": f"å·²åˆ é™¤æ–‡æ¡£åŠ {deleted_count} ä¸ªçŸ¥è¯†æ¡ç›®"
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

@app.get("/documents/{doc_id}/items", summary="è·å–æ–‡æ¡£ä¸‹çš„çŸ¥è¯†æ¡ç›®")
async def get_document_items(doc_id: str, page: int = 1, page_size: int = 20):
    """è·å–æŒ‡å®šæ–‡æ¡£ä¸‹çš„æ‰€æœ‰çŸ¥è¯†æ¡ç›®ï¼ˆåˆ†é¡µï¼‰"""
    return await get_knowledge_items(doc_id=doc_id, page=page, page_size=page_size)


# å¼‚å¸¸å¤„ç†
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """å…¨å±€å¼‚å¸¸å¤„ç†"""
    return JSONResponse(
        status_code=500,
        content={
            "success": False,
            "error": str(exc),
            "timestamp": datetime.now().isoformat()
        }
    )

if __name__ == "__main__":
    print(f"ğŸš€ å¯åŠ¨ {Config.API_TITLE}")
    print(f"ğŸ“ æœåŠ¡åœ°å€: http://{Config.API_HOST}:{Config.API_PORT}")
    print(f"ğŸ“š APIæ–‡æ¡£: http://{Config.API_HOST}:{Config.API_PORT}/docs")
    
    uvicorn.run(
        "retrieval_api:app",
        host=Config.API_HOST,
        port=Config.API_PORT,
        reload=False,
        workers=1  # ä½¿ç”¨å•workerç¡®ä¿æ¨¡å‹åªåŠ è½½ä¸€æ¬¡
    ) 