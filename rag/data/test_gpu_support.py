#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BGEæ¨¡å‹GPUæ”¯æŒæµ‹è¯•è„šæœ¬
"""

import torch
import time
import numpy as np

def check_gpu_environment():
    """æ£€æŸ¥GPUç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥GPUç¯å¢ƒ...")
    print("=" * 50)
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    cuda_available = torch.cuda.is_available()
    print(f"CUDAå¯ç”¨: {'âœ… æ˜¯' if cuda_available else 'âŒ å¦'}")
    
    if cuda_available:
        # æ˜¾ç¤ºGPUä¿¡æ¯
        gpu_count = torch.cuda.device_count()
        print(f"GPUæ•°é‡: {gpu_count}")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            gpu_compute = torch.cuda.get_device_properties(i).major
            print(f"GPU {i}: {gpu_name}")
            print(f"       æ˜¾å­˜: {gpu_memory:.1f}GB")
            print(f"       è®¡ç®—èƒ½åŠ›: {gpu_compute}.x")
        
        # æ£€æŸ¥å½“å‰GPUä½¿ç”¨æƒ…å†µ
        current_device = torch.cuda.current_device()
        memory_allocated = torch.cuda.memory_allocated(current_device) / (1024**2)
        memory_cached = torch.cuda.memory_reserved(current_device) / (1024**2)
        
        print(f"å½“å‰è®¾å¤‡: GPU {current_device}")
        print(f"å·²åˆ†é…å†…å­˜: {memory_allocated:.1f}MB")
        print(f"ç¼“å­˜å†…å­˜: {memory_cached:.1f}MB")
        
        return True, gpu_count
    else:
        print("ğŸ’¡ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")
        print("ğŸ“ å¦‚éœ€GPUæ”¯æŒï¼Œè¯·å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch")
        return False, 0

def test_bge_with_gpu():
    """æµ‹è¯•BGEæ¨¡å‹çš„GPUæ”¯æŒ"""
    print("\nğŸ§ª æµ‹è¯•BGEæ¨¡å‹GPUæ”¯æŒ...")
    print("=" * 50)
    
    try:
        from sentence_transformers import SentenceTransformer
        
        model_path = "models/AI-ModelScope/bge-large-zh-v1___5"
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "ç³–å°¿ç—…çš„ä¸»è¦ç—‡çŠ¶æœ‰å“ªäº›ï¼Ÿ",
            "å¦‚ä½•é¢„é˜²é«˜è¡€å‹ï¼Ÿ",
            "å¥åº·é¥®é£Ÿçš„å»ºè®®",
            "è¿åŠ¨å¯¹èº«ä½“å¥åº·çš„å¥½å¤„",
            "ç³–å°¿ç—…è§†ç½‘è†œç—…å˜çš„æ²»ç–—æ–¹æ³•",
            "è¡€ç³–æ§åˆ¶çš„é‡è¦æ€§"
        ]
        
        results = {}
        
        # CPUæµ‹è¯•
        print("\n--- CPUæ€§èƒ½æµ‹è¯• ---")
        print("ğŸ”„ åŠ è½½æ¨¡å‹åˆ°CPU...")
        start_time = time.time()
        cpu_model = SentenceTransformer(model_path, device='cpu')
        cpu_load_time = time.time() - start_time
        print(f"âœ… CPUæ¨¡å‹åŠ è½½å®Œæˆ: {cpu_load_time:.2f}ç§’")
        
        # CPUç¼–ç æµ‹è¯•
        print("ğŸ§  CPUæ–‡æœ¬ç¼–ç æµ‹è¯•...")
        start_time = time.time()
        cpu_embeddings = cpu_model.encode(test_texts)
        cpu_encode_time = time.time() - start_time
        print(f"âœ… CPUç¼–ç å®Œæˆ: {cpu_encode_time:.3f}ç§’")
        print(f"ğŸ“ å‘é‡å½¢çŠ¶: {cpu_embeddings.shape}")
        
        results['CPU'] = {
            'load_time': cpu_load_time,
            'encode_time': cpu_encode_time,
            'total_time': cpu_load_time + cpu_encode_time
        }
        
        # GPUæµ‹è¯•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if torch.cuda.is_available():
            print("\n--- GPUæ€§èƒ½æµ‹è¯• ---")
            print("ğŸ”„ åŠ è½½æ¨¡å‹åˆ°GPU...")
            start_time = time.time()
            gpu_model = SentenceTransformer(model_path, device='cuda')
            gpu_load_time = time.time() - start_time
            print(f"âœ… GPUæ¨¡å‹åŠ è½½å®Œæˆ: {gpu_load_time:.2f}ç§’")
            
            # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨
            memory_allocated = torch.cuda.memory_allocated() / (1024**2)
            print(f"ğŸ’¾ GPUå†…å­˜ä½¿ç”¨: {memory_allocated:.1f}MB")
            
            # GPUé¢„çƒ­
            print("ğŸ”¥ GPUé¢„çƒ­...")
            gpu_model.encode(["é¢„çƒ­æ–‡æœ¬"])
            torch.cuda.synchronize()
            
            # GPUç¼–ç æµ‹è¯•
            print("ğŸ§  GPUæ–‡æœ¬ç¼–ç æµ‹è¯•...")
            start_time = time.time()
            gpu_embeddings = gpu_model.encode(test_texts)
            torch.cuda.synchronize()  # ç¡®ä¿GPUæ“ä½œå®Œæˆ
            gpu_encode_time = time.time() - start_time
            print(f"âœ… GPUç¼–ç å®Œæˆ: {gpu_encode_time:.3f}ç§’")
            print(f"ğŸ“ å‘é‡å½¢çŠ¶: {gpu_embeddings.shape}")
            
            # æ˜¾ç¤ºæœ€ç»ˆGPUå†…å­˜ä½¿ç”¨
            memory_allocated = torch.cuda.memory_allocated() / (1024**2)
            memory_cached = torch.cuda.memory_reserved() / (1024**2)
            print(f"ğŸ’¾ GPUå†…å­˜ä½¿ç”¨: {memory_allocated:.1f}MB (åˆ†é…) + {memory_cached:.1f}MB (ç¼“å­˜)")
            
            results['GPU'] = {
                'load_time': gpu_load_time,
                'encode_time': gpu_encode_time,
                'total_time': gpu_load_time + gpu_encode_time
            }
            
            # éªŒè¯ç»“æœä¸€è‡´æ€§
            print("\nğŸ” éªŒè¯CPUä¸GPUç»“æœä¸€è‡´æ€§...")
            max_diff = np.max(np.abs(cpu_embeddings - gpu_embeddings))
            print(f"æœ€å¤§å·®å¼‚: {max_diff:.6f}")
            
            if max_diff < 1e-4:
                print("âœ… CPUä¸GPUç»“æœé«˜åº¦ä¸€è‡´")
            elif max_diff < 1e-2:
                print("âš ï¸  CPUä¸GPUç»“æœæœ‰è½»å¾®å·®å¼‚ (æ­£å¸¸)")
            else:
                print("âŒ CPUä¸GPUç»“æœå·®å¼‚è¾ƒå¤§")
        
        # æ€§èƒ½å¯¹æ¯”
        if len(results) > 1:
            print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
            print("=" * 60)
            print(f"{'è®¾å¤‡':>6} | {'åŠ è½½æ—¶é—´':>8} | {'ç¼–ç æ—¶é—´':>8} | {'æ€»æ—¶é—´':>8}")
            print("-" * 60)
            
            for device, times in results.items():
                print(f"{device:>6} | {times['load_time']:>7.2f}s | {times['encode_time']:>7.3f}s | {times['total_time']:>7.2f}s")
            
            if 'CPU' in results and 'GPU' in results:
                speedup_encode = results['CPU']['encode_time'] / results['GPU']['encode_time']
                speedup_total = results['CPU']['total_time'] / results['GPU']['total_time']
                
                print(f"\nğŸš€ GPUåŠ é€Ÿæ•ˆæœ:")
                print(f"   ç¼–ç åŠ é€Ÿæ¯”: {speedup_encode:.2f}x")
                print(f"   æ€»ä½“åŠ é€Ÿæ¯”: {speedup_total:.2f}x")
                
                if speedup_encode > 2.0:
                    print("ğŸ‰ GPUæ˜¾è‘—æå‡ç¼–ç æ€§èƒ½!")
                elif speedup_encode > 1.3:
                    print("ğŸ’¡ GPUæœ‰æ˜æ˜¾æ€§èƒ½æå‡")
                elif speedup_encode > 1.1:
                    print("ğŸ“ˆ GPUæœ‰è½»å¾®æ€§èƒ½æå‡")
                else:
                    print("âš ï¸  GPUæ€§èƒ½æå‡ä¸æ˜æ˜¾")
                    print("   å¯èƒ½åŸå› : æ•°æ®é‡è¾ƒå°ã€GPUé¢„çƒ­ä¸è¶³æˆ–é©±åŠ¨é—®é¢˜")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ BGE-large-zh-v1.5 GPUæ”¯æŒæµ‹è¯•")
    print("=" * 60)
    
    # GPUç¯å¢ƒæ£€æµ‹
    cuda_available, gpu_count = check_gpu_environment()
    
    # BGEæ¨¡å‹GPUæµ‹è¯•
    if test_bge_with_gpu():
        print("\n" + "=" * 60)
        print("ğŸ‰ BGEæ¨¡å‹GPUæµ‹è¯•å®Œæˆ!")
        
        if cuda_available:
            print("âœ… GPUæ”¯æŒæ­£å¸¸ï¼Œå¯ç”¨äºç”Ÿäº§ç¯å¢ƒ")
            print("ğŸ’¡ å»ºè®®åœ¨å‘é‡åŒ–å¤§é‡æ•°æ®æ—¶ä½¿ç”¨GPUåŠ é€Ÿ")
        else:
            print("ğŸ’» å½“å‰ä¸ºCPUæ¨¡å¼ï¼Œæ€§èƒ½å¯èƒ½è¾ƒæ…¢")
            print("ğŸ“ å¦‚éœ€GPUåŠ é€Ÿï¼Œè¯·å®‰è£…CUDAå’Œå¯¹åº”çš„PyTorchç‰ˆæœ¬")
            
        print("=" * 60)
        return True
    else:
        print("\nâŒ GPUæµ‹è¯•å¤±è´¥!")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1) 