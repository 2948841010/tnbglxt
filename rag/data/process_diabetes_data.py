#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç³–å°¿ç—…çŸ¥è¯†åº“æ•°æ®æ¸…æ´—å’Œæ–‡æœ¬åˆ†å—å¤„ç†è„šæœ¬
æ ¹æ®RAG_Retrieval_Service_Plan.mdå®ç°æ•°æ®é¢„å¤„ç†æµç¨‹
"""

import pandas as pd
import json
import re
import os
from typing import List, Dict, Tuple, Optional
from pathlib import Path
import hashlib
from datetime import datetime
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('data_processing.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class DiabetesDataProcessor:
    """ç³–å°¿ç—…çŸ¥è¯†åº“æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, csv_file: str, output_dir: str = "processed_data"):
        self.csv_file = csv_file
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åŒ»å­¦æœ¯è¯­æ ‡å‡†åŒ–è¯å…¸
        self.medical_terms = {
            # ç³–å°¿ç—…ç›¸å…³æœ¯è¯­
            "ç³–å°¿ç—…": ["ç³–å°¿ç—…", "DM", "diabetes"],
            "1å‹ç³–å°¿ç—…": ["1å‹ç³–å°¿ç—…", "Iå‹ç³–å°¿ç—…", "èƒ°å²›ç´ ä¾èµ–å‹ç³–å°¿ç—…", "IDDM"],
            "2å‹ç³–å°¿ç—…": ["2å‹ç³–å°¿ç—…", "IIå‹ç³–å°¿ç—…", "éèƒ°å²›ç´ ä¾èµ–å‹ç³–å°¿ç—…", "NIDDM"],
            "è¡€ç³–": ["è¡€ç³–", "è¡€ç³–å€¼", "è¡€ç³–æ°´å¹³", "glucose"],
            "èƒ°å²›ç´ ": ["èƒ°å²›ç´ ", "insulin"],
            "å¹¶å‘ç—‡": ["å¹¶å‘ç—‡", "åˆå¹¶ç—‡"],
            # çœ¼éƒ¨ç–¾ç—…æœ¯è¯­
            "è§†ç½‘è†œç—…å˜": ["è§†ç½‘è†œç—…å˜", "retinopathy"],
            "é»„æ–‘æ°´è‚¿": ["é»„æ–‘æ°´è‚¿", "macular edema"],
            "è§†åŠ›ä¸‹é™": ["è§†åŠ›ä¸‹é™", "è§†åŠ›å‡é€€", "è§†åŠ›æ¨¡ç³Š"],
            "é£èšŠç—‡": ["é£èšŠç—‡", "é£èšŠ", "çœ¼å‰é£èšŠ"],
        }
        
        # ç–¾ç—…åˆ†ç±»
        self.disease_categories = {
            "çœ¼éƒ¨ç–¾ç—…": ["è§†ç½‘è†œç—…å˜", "é»„æ–‘æ°´è‚¿", "ç»ç’ƒä½“å‡ºè¡€", "è§†ç½‘è†œè„±ç¦»"],
            "è‚¾è„ç–¾ç—…": ["ç³–å°¿ç—…è‚¾ç—…", "è‚¾åŠŸèƒ½ä¸å…¨"],
            "ç¥ç»ç–¾ç—…": ["ç³–å°¿ç—…ç¥ç»ç—…å˜", "å‘¨å›´ç¥ç»ç—…å˜"],
            "å¿ƒè¡€ç®¡ç–¾ç—…": ["å¿ƒè¡€ç®¡å¹¶å‘ç—‡", "å† å¿ƒç—…"],
            "åŸºç¡€çŸ¥è¯†": ["ç³–å°¿ç—…å®šä¹‰", "ç—…å› ", "åˆ†ç±»"],
            "æ²»ç–—æ–¹æ³•": ["è¯ç‰©æ²»ç–—", "æ‰‹æœ¯æ²»ç–—", "æ¿€å…‰æ²»ç–—"],
            "è¯Šæ–­æ£€æŸ¥": ["çœ¼åº•æ£€æŸ¥", "è¡€ç®¡é€ å½±", "è¶…å£°æ£€æŸ¥"]
        }
    
    def clean_text(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬å†…å®¹"""
        if not text or pd.isna(text):
            return ""
        
        # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', str(text).strip())
        
        # ç§»é™¤HTMLæ ‡ç­¾ï¼ˆå¦‚æœæœ‰ï¼‰
        text = re.sub(r'<[^>]+>', '', text)
        
        # è§„èŒƒåŒ–æ ‡ç‚¹ç¬¦å·
        text = text.replace('ï¼Ÿ', 'ï¼Ÿ').replace('ï¼', 'ï¼').replace('ï¼Œ', 'ï¼Œ')
        
        # ç§»é™¤é‡å¤çš„æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'([ã€‚ï¼ï¼Ÿï¼Œ])\1+', r'\1', text)
        
        return text.strip()
    
    def standardize_medical_terms(self, text: str) -> str:
        """æ ‡å‡†åŒ–åŒ»å­¦æœ¯è¯­"""
        for standard_term, variants in self.medical_terms.items():
            for variant in variants:
                if variant != standard_term:
                    text = text.replace(variant, standard_term)
        return text
    
    def extract_category(self, question: str, context: str) -> str:
        """ä»é—®é¢˜å’Œä¸Šä¸‹æ–‡ä¸­æå–ç–¾ç—…åˆ†ç±»"""
        combined_text = f"{question} {context}".lower()
        
        for category, keywords in self.disease_categories.items():
            for keyword in keywords:
                if keyword.lower() in combined_text:
                    return category
        
        return "å…¶ä»–"
    
    def extract_medical_entities(self, text: str) -> List[str]:
        """æå–åŒ»å­¦å®ä½“"""
        entities = []
        text_lower = text.lower()
        
        for standard_term, variants in self.medical_terms.items():
            for variant in variants:
                if variant.lower() in text_lower:
                    if standard_term not in entities:
                        entities.append(standard_term)
                    break
        
        return entities
    
    def chunk_long_text(self, text: str, max_length: int = 500) -> List[str]:
        """å¯¹é•¿æ–‡æœ¬è¿›è¡Œåˆ†å—å¤„ç†"""
        if len(text) <= max_length:
            return [text]
        
        chunks = []
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            sentence += "ã€‚"  # æ¢å¤æ ‡ç‚¹
            
            if len(current_chunk + sentence) <= max_length:
                current_chunk += sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks if chunks else [text]
    
    def generate_chunk_id(self, text: str, index: int = 0) -> str:
        """ç”Ÿæˆå—çš„å”¯ä¸€ID"""
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()[:8]
        return f"chunk_{text_hash}_{index}"
    
    def process_qa_pair(self, question: str, context: str, row_index: int) -> List[Dict]:
        """å¤„ç†å•ä¸ªQAå¯¹"""
        # æ¸…æ´—æ–‡æœ¬
        question = self.clean_text(question)
        context = self.clean_text(context)
        
        if not question or not context:
            return []
        
        # æ ‡å‡†åŒ–åŒ»å­¦æœ¯è¯­
        question = self.standardize_medical_terms(question)
        context = self.standardize_medical_terms(context)
        
        # æå–åˆ†ç±»å’Œå®ä½“
        category = self.extract_category(question, context)
        entities = self.extract_medical_entities(f"{question} {context}")
        
        # åˆ†å—å¤„ç†contextï¼ˆå¦‚æœå¤ªé•¿ï¼‰
        context_chunks = self.chunk_long_text(context)
        
        processed_chunks = []
        
        for chunk_idx, chunk in enumerate(context_chunks):
            chunk_data = {
                "id": self.generate_chunk_id(f"{question}_{chunk}", chunk_idx),
                "question": question,
                "context": chunk,
                "category": category,
                "entities": entities,
                "source_row": row_index,
                "chunk_index": chunk_idx,
                "total_chunks": len(context_chunks),
                "text_length": len(chunk),
                "processed_at": datetime.now().isoformat()
            }
            processed_chunks.append(chunk_data)
        
        return processed_chunks
    
    def detect_duplicates(self, data: List[Dict]) -> List[int]:
        """æ£€æµ‹é‡å¤å†…å®¹"""
        seen_content = set()
        duplicates = []
        
        for i, item in enumerate(data):
            content_key = f"{item['question']}_{item['context']}"
            content_hash = hashlib.md5(content_key.encode('utf-8')).hexdigest()
            
            if content_hash in seen_content:
                duplicates.append(i)
            else:
                seen_content.add(content_hash)
        
        return duplicates
    
    def validate_data_quality(self, data: List[Dict]) -> Dict:
        """éªŒè¯æ•°æ®è´¨é‡"""
        stats = {
            "total_chunks": len(data),
            "empty_questions": 0,
            "empty_contexts": 0,
            "short_contexts": 0,  # å°‘äº10ä¸ªå­—ç¬¦
            "long_contexts": 0,   # è¶…è¿‡1000ä¸ªå­—ç¬¦
            "categories": {},
            "avg_text_length": 0,
            "duplicates": 0
        }
        
        total_length = 0
        
        for item in data:
            # æ£€æŸ¥ç©ºå†…å®¹
            if not item["question"].strip():
                stats["empty_questions"] += 1
            if not item["context"].strip():
                stats["empty_contexts"] += 1
            
            # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
            context_len = len(item["context"])
            total_length += context_len
            
            if context_len < 10:
                stats["short_contexts"] += 1
            elif context_len > 1000:
                stats["long_contexts"] += 1
            
            # ç»Ÿè®¡åˆ†ç±»
            category = item["category"]
            stats["categories"][category] = stats["categories"].get(category, 0) + 1
        
        # è®¡ç®—å¹³å‡é•¿åº¦
        if data:
            stats["avg_text_length"] = total_length / len(data)
        
        # æ£€æµ‹é‡å¤
        duplicates = self.detect_duplicates(data)
        stats["duplicates"] = len(duplicates)
        
        return stats, duplicates
    
    def process_csv(self) -> Tuple[List[Dict], Dict]:
        """å¤„ç†CSVæ–‡ä»¶"""
        logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {self.csv_file}")
        
        # è¯»å–CSVæ–‡ä»¶
        try:
            df = pd.read_csv(self.csv_file, encoding='utf-8')
            logger.info(f"æˆåŠŸè¯»å–CSVæ–‡ä»¶ï¼Œå…± {len(df)} è¡Œæ•°æ®")
        except Exception as e:
            logger.error(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
            raise
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—
        if 'question' not in df.columns or 'context' not in df.columns:
            raise ValueError("CSVæ–‡ä»¶å¿…é¡»åŒ…å« 'question' å’Œ 'context' åˆ—")
        
        # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
        all_chunks = []
        processed_count = 0
        error_count = 0
        
        for index, row in df.iterrows():
            try:
                chunks = self.process_qa_pair(
                    row['question'], 
                    row['context'], 
                    index
                )
                all_chunks.extend(chunks)
                processed_count += len(chunks)
                
                if index % 100 == 0:
                    logger.info(f"å·²å¤„ç† {index} è¡Œï¼Œç”Ÿæˆ {processed_count} ä¸ªæ•°æ®å—")
                    
            except Exception as e:
                logger.error(f"å¤„ç†ç¬¬ {index} è¡Œæ—¶å‡ºé”™: {e}")
                error_count += 1
                continue
        
        logger.info(f"æ•°æ®å¤„ç†å®Œæˆ: å¤„ç† {len(df)} è¡Œï¼Œç”Ÿæˆ {len(all_chunks)} ä¸ªæ•°æ®å—ï¼Œé”™è¯¯ {error_count} ä¸ª")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        quality_stats, duplicates = self.validate_data_quality(all_chunks)
        
        # ç§»é™¤é‡å¤é¡¹
        if duplicates:
            logger.info(f"å‘ç° {len(duplicates)} ä¸ªé‡å¤é¡¹ï¼Œæ­£åœ¨ç§»é™¤...")
            all_chunks = [chunk for i, chunk in enumerate(all_chunks) if i not in duplicates]
            quality_stats["total_chunks"] = len(all_chunks)
        
        return all_chunks, quality_stats
    
    def save_processed_data(self, data: List[Dict], stats: Dict):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        data_file = self.output_dir / "diabetes_qa_processed.json"
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"å¤„ç†åçš„æ•°æ®å·²ä¿å­˜åˆ°: {data_file}")
        
        # ä¿å­˜è´¨é‡ç»Ÿè®¡
        stats_file = self.output_dir / "data_quality_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        logger.info(f"æ•°æ®è´¨é‡ç»Ÿè®¡å·²ä¿å­˜åˆ°: {stats_file}")
        
        # ç”Ÿæˆå¯è¯»çš„ç»Ÿè®¡æŠ¥å‘Š
        report_file = self.output_dir / "processing_report.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("ç³–å°¿ç—…çŸ¥è¯†åº“æ•°æ®å¤„ç†æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ•°æ®æ€»é‡: {stats['total_chunks']} ä¸ªæ•°æ®å—\n")
            f.write(f"å¹³å‡æ–‡æœ¬é•¿åº¦: {stats['avg_text_length']:.1f} å­—ç¬¦\n")
            f.write(f"é‡å¤é¡¹: {stats['duplicates']} ä¸ª\n")
            f.write(f"ç©ºé—®é¢˜: {stats['empty_questions']} ä¸ª\n")
            f.write(f"ç©ºç­”æ¡ˆ: {stats['empty_contexts']} ä¸ª\n")
            f.write(f"çŸ­æ–‡æœ¬: {stats['short_contexts']} ä¸ª (< 10å­—ç¬¦)\n")
            f.write(f"é•¿æ–‡æœ¬: {stats['long_contexts']} ä¸ª (> 1000å­—ç¬¦)\n\n")
            
            f.write("åˆ†ç±»åˆ†å¸ƒ:\n")
            for category, count in stats['categories'].items():
                percentage = (count / stats['total_chunks']) * 100
                f.write(f"  {category}: {count} ä¸ª ({percentage:.1f}%)\n")
        
        logger.info(f"å¤„ç†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        return data_file, stats_file, report_file


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ§¹ ç³–å°¿ç—…çŸ¥è¯†åº“æ•°æ®æ¸…æ´—å’Œæ–‡æœ¬åˆ†å—")
    print("=" * 60)
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = DiabetesDataProcessor("diabetes_qa_chinese.csv")
    
    try:
        # å¤„ç†æ•°æ®
        processed_data, quality_stats = processor.process_csv()
        
        # ä¿å­˜æ•°æ®
        data_file, stats_file, report_file = processor.save_processed_data(
            processed_data, quality_stats
        )
        
        print("\nâœ… æ•°æ®å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š æ€»æ•°æ®å—: {quality_stats['total_chunks']}")
        print(f"ğŸ“ˆ å¹³å‡é•¿åº¦: {quality_stats['avg_text_length']:.1f} å­—ç¬¦")
        print(f"ğŸ—‚ï¸  åˆ†ç±»æ•°é‡: {len(quality_stats['categories'])}")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   - å¤„ç†æ•°æ®: {data_file}")
        print(f"   - è´¨é‡ç»Ÿè®¡: {stats_file}")
        print(f"   - å¤„ç†æŠ¥å‘Š: {report_file}")
        
        print("\nğŸ“‹ åˆ†ç±»åˆ†å¸ƒ:")
        for category, count in quality_stats['categories'].items():
            percentage = (count / quality_stats['total_chunks']) * 100
            print(f"   {category}: {count} ä¸ª ({percentage:.1f}%)")
        
    except Exception as e:
        logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        return False
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ•°æ®æ¸…æ´—å’Œåˆ†å—å®Œæˆ!")
    print("ğŸ“ ä¸‹ä¸€æ­¥: è¿›è¡Œå‘é‡åŒ–å¤„ç†")
    print("=" * 60)
    
    return True


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1) 